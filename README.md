# Projeto de ETL com Apache Airflow

Este projeto Ã© uma exploraÃ§Ã£o prÃ¡tica do **Apache Airflow** aplicado a tarefas de **ETL (Extract, Transform, Load)**. Ele utiliza diferentes operadores para manipulaÃ§Ã£o e orquestraÃ§Ã£o de dados, alÃ©m de integraÃ§Ã£o com serviÃ§os como **MinIO**, **PostgreSQL**, **MariaDB** e **Metabase** para visualizaÃ§Ã£o de dados.

> âš ï¸ Este repositÃ³rio Ã© baseado em um fork, mas sofreu modificaÃ§Ãµes significativas com foco em aprendizado, testes e integraÃ§Ã£o com mÃºltiplos serviÃ§os.

---

## ğŸš€ Objetivo

Demonstrar o uso de operadores do Airflow para construir pipelines ETL, com apoio de um ambiente Docker para orquestraÃ§Ã£o dos serviÃ§os envolvidos.

---

## ğŸ—‚ï¸ Estrutura do Projeto

```
â”œâ”€â”€ acesso_interface__airflow.txt
â”œâ”€â”€ airflow
â”‚Â Â  â”œâ”€â”€ config_airflow
â”‚Â Â  â”‚Â Â  â””â”€â”€ airflow.Dockerfile
â”‚Â Â  â””â”€â”€ dags
â”‚Â Â      â”œâ”€â”€ etl_with_views_dag.py         # DAG principal para processo ETL com views
â”‚Â Â      â””â”€â”€ operator_test.py              # DAG auxiliar para testes com operadores
â”œâ”€â”€ docker-compose.yml                    # OrquestraÃ§Ã£o completa dos serviÃ§os
â”œâ”€â”€ requirements.txt                      # DependÃªncias Python para o Airflow
```

---

## ğŸ³ ServiÃ§os via Docker Compose

- **Airflow** (com LocalExecutor)
- **PostgreSQL** (para Airflow)
- **MariaDB** (banco de dados de origem)
- **MinIO** (armazenamento de dados simulando S3)
- **Metabase** (ferramenta de BI para visualizaÃ§Ã£o)
- **PostgreSQL Metabase** (banco interno do Metabase)

---

## âš™ï¸ Como executar

1. Clone o projeto:
   ```bash
   git clone <url-do-seu-repo>
   cd <nome-do-projeto>
   ```

2. Suba os serviÃ§os com Docker Compose:
   ```bash
   docker-compose up --build
   ```

3. Acesse as interfaces:
   - Airflow: [http://localhost:8080](http://localhost:8080)
   - MinIO: [http://localhost:9001](http://localhost:9001) (login: `minioadmin` / `minio@1234!`)
   - Metabase: [http://localhost:3000](http://localhost:3000)

---

## ğŸ“Š Casos de Uso

- Criar pipelines com mÃºltiplos operadores (PythonOperator, BashOperator, etc).
- Utilizar banco de dados relacional (MariaDB) como origem de dados.
- Salvar dados transformados em bucket MinIO (simulando S3).
- Visualizar os dados processados via Metabase.

---

## ğŸ“Œ ObservaÃ§Ãµes

- O DAG `etl_with_views_dag.py` Ã© o principal fluxo de ETL.
- Os arquivos `.txt` como `acesso_interface__airflow.txt` servem como dados de entrada ou documentaÃ§Ã£o complementar.
- O projeto estÃ¡ em constante evoluÃ§Ã£o para testes e estudo do Airflow em ambientes reais.

---

## ğŸ› ï¸ Tecnologias Utilizadas

- [Apache Airflow](https://airflow.apache.org/)
- [Docker + Docker Compose](https://docs.docker.com/)
- [MinIO](https://min.io/)
- [MariaDB](https://mariadb.org/)
- [PostgreSQL](https://www.postgresql.org/)
- [Metabase](https://www.metabase.com/)

---

## ğŸ“„ LicenÃ§a

Este projeto Ã© livre para fins educacionais e pode ser modificado conforme necessÃ¡rio.

---

## âœ¨ CrÃ©ditos

Este projeto foi originalmente baseado em um fork de outro repositÃ³rio com propÃ³sitos semelhantes, mas passou por modificaÃ§Ãµes estruturais e de conteÃºdo.
